import asyncio
import copy
import concurrent.futures
import logging
import time
import traceback
import uuid
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import openai
import torch
from openai.types import Completion

from rllm.agents.agent import Action, BaseAgent, Step, Trajectory
from rllm.agents.utils import (
    convert_messages_to_tokens_and_masks,
    get_recent_assistant_user_messages,
)
from rllm.environments.base.base_env import BaseEnv
from rllm.environments.env_utils import (
    compute_mc_return,
    compute_trajectory_reward,
)
from rllm.misc import colorful_print
from rllm.parser.chat_template.parser import ChatTemplateParser
from rllm.router.router import Router

logger = logging.getLogger(__name__)


class AgentExecutionEngine:
    def __init__(
        self,
        engine_name="openai",
        tokenizer=None,
        rollout_engine=None,
        chat_parser=None,
        n_parallel_agents=1,
        trajectory_timeout=None,
        gamma=0.2,
        api_retries=3,
        retry_limit=3,
        max_steps=5,
        max_response_length=8192,
        max_prompt_length=1024,
        config=None,
        agent_class=None,
        env_class=None,
        agent_args=None,
        rollout_engine_args=None,
        env_args=None,
        max_workers=64,
        enforce_max_prompt_length=False,  # If enabled, applies max_prompt check per step
        overlong_filter=False,  # Filter for overlong trajectories (i.e. TRUNCATION, MAX_STEPS, TIMEOUT)
        **kwargs,
    ):
        if agent_args is None:
            agent_args = {}
        if rollout_engine_args is None:
            rollout_engine_args = {}
        if env_args is None:
            env_args = {}

        self.config = config
        self.rollout_engine = rollout_engine
        self.tokenizer = tokenizer
        self.engine_name = engine_name
        self.n_parallel_agents = n_parallel_agents
        self.overlong_filter = overlong_filter

        # For interaction
        self.gamma = gamma
        self.retry_limit = retry_limit
        self.api_retries = api_retries
        self.max_steps = max_steps
        self.max_response_length = max_response_length
        self.max_prompt_length = max_prompt_length
        self.enforce_max_prompt_length = enforce_max_prompt_length

        self.agent_class = agent_class
        self.agent_args = agent_args
        self.env_class = env_class
        self.env_args = env_args

        self.agents = [None for _ in range(n_parallel_agents)]
        self.envs = [None for _ in range(n_parallel_agents)]

        self.trajectory_timeout = trajectory_timeout
        if not trajectory_timeout:
            self.trajectory_timeout = int(1e9)

        if env_class is not None:
            assert env_class.is_multithread_safe(), "Environment must be multithread safe for async engine"
        # rollout engine args
        self.rollout_engine_args = rollout_engine_args
        self.sampling_params = kwargs.get("sampling_params", {})

        assert self.engine_name in ["openai", "verl"], "Currently only openai and verl are supported as rollout engine"
        if self.engine_name == "openai":
            from openai import AsyncOpenAI

            self.client = AsyncOpenAI(**self.rollout_engine_args)
            # Disable httpx INFO logs that show HTTP requests
            logging.getLogger("httpx").setLevel(logging.WARNING)
        elif self.engine_name == "verl":
            # All generation is done via scheduler. Currently only works for verl
            self.server_addresses = getattr(self.rollout_engine, "server_addresses", [])
            self.router = Router(config=self.config, tokenizer=self.tokenizer, addresses=self.server_addresses)

        # Create a thread pool executor for environment interactions (i.e. step, reset, close)
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)

        if chat_parser is None:
            self.chat_parser = ChatTemplateParser.get_parser(self.tokenizer, disable_thinking=kwargs.get("disable_thinking", False))
        else:
            self.chat_parser = chat_parser

    async def get_model_response(self, prompt, application_id, **kwargs):
        """
        Compute model response asynchronously based on the engine type.

        This function is multithread safe and routes the request to the appropriate
        engine-specific handler.

        Args:
            prompt: The input prompt to send to the model
            application_id: Unique identifier for the application
            **kwargs: Additional arguments to pass to the model

        Returns:
            The model's response text

        Raises:
            NotImplementedError: If the engine type is not supported
        """
        # print("PROMPT: ", prompt)
        kwargs.setdefault("logprobs", True)
        kwargs.setdefault("top_logprobs", 1)
        
        if self.engine_name == "openai":
            lp = kwargs.get("logprobs", None)
            if isinstance(lp, bool):
                kwargs["logprobs"] = 1 if lp else None  # int or None
            # not valid for Completions requests; drop it if present
            kwargs.pop("top_logprobs", None)
            # don't send Nones down to the SDK
            kwargs = {k: v for k, v in kwargs.items() if v is not None}
            return await self._get_openai_async(prompt, application_id, **kwargs)
        elif self.engine_name == "verl":
            return await self._get_verl_async(prompt, application_id, **kwargs)
        else:
            raise NotImplementedError(f"Engine type '{self.engine_name}' not supported")

    def update_envs_and_agents(self, envs, agents):
        """
        Update the environments and agents.

        Args:
            envs: List of environments to use
            agents: List of agents to use
        """
        assert len(agents) == len(envs), f"Number of agents must equal to number of environments but received, {len(agents)} and {len(envs)}"
        self.envs = envs
        # For keeping track of the environment index in the batch.
        for idx, env in enumerate(envs):
            env.idx = idx
        self.agents = agents
        self.n_parallel_agents = len(envs)

    async def _get_verl_async(self, prompt, application_id, **kwargs):
        batch = self._convert_prompt_verl([prompt], **kwargs)

        if "max_tokens" in kwargs:
            batch.meta_info["max_tokens"] = kwargs["max_tokens"]

        # ask the router to return logprobs for the rollout policy if it can
        batch.meta_info.setdefault("recompute_log_prob", True)
        batch.meta_info.setdefault("return_logprob", True)

        output = await self.router.generate_sequences(batch, application_id=application_id, **kwargs)

        # Attention & tokens over the response window (after max_prompt_length)
        attn = output.batch["attention_mask"][0, self.max_prompt_length:]
        tokens = output.batch["responses"][0]

        # Find last index where attention == 1 (end of response)
        non_pad_indices = (attn == 1).nonzero(as_tuple=True)[0]
        if len(non_pad_indices) == 0:
            trimmed = tokens[:0]  # empty
            resp_len = 0
        else:
            last_valid_idx = non_pad_indices[-1].item()
            trimmed = tokens[: last_valid_idx + 1]  # include the last valid token
            resp_len = last_valid_idx + 1

        # Decode response (preserve internal specials; strip pad/eos from text like your original)
        response = self.tokenizer.decode(trimmed, skip_special_tokens=False)
        pad_token = getattr(self.tokenizer, "pad_token", "")
        eos_token = getattr(self.tokenizer, "eos_token", "")
        if pad_token:
            response = response.replace(pad_token, "")
        if eos_token:
            response = response.replace(eos_token, "")

        # --- rollout logprobs (Î¸_vllm) aligned to response tokens ---
        response_logprobs = []
        lp_tensor = None

        # Prefer native rollout key if present; fall back to legacy if router names it differently
        if "rollout_log_probs" in output.batch:
            lp_tensor = output.batch["rollout_log_probs"][0]  # shape: [prompt+resp] or [resp]
        elif "old_log_probs" in output.batch:
            # Some backends still use this key; treat as rollout logprobs at collection time
            lp_tensor = output.batch["old_log_probs"][0]

        if lp_tensor is not None:
            # If the tensor is full-length (prompt + response), slice the response region.
            if lp_tensor.shape[0] == output.batch["attention_mask"].shape[1]:
                lp_resp = lp_tensor[self.max_prompt_length : self.max_prompt_length + resp_len]
            else:
                # Already response-only; just trim/pad to resp_len
                lp_resp = lp_tensor[:resp_len]
            response_logprobs = lp_resp.detach().cpu().tolist()

        return {"text": response, "response_logprobs": response_logprobs}


    async def _get_openai_async(self, prompt, _, **kwargs):
        """
        Get action from OpenAI API asynchronously with retry logic.

        Args:
            prompt: The input prompt in text format for completions API
            application_id: Unique identifier for the application (unused for OpenAI)
            **kwargs: Additional arguments to pass to the OpenAI API

        Returns:
            The response from OpenAI API
        """

        async def get_response(prompt_text: str):
            retries = self.api_retries
            while retries > 0:
                try:
                    response = await self.client.completions.create(
                        prompt=prompt_text,
                        timeout=3600,
                        **self.sampling_params,
                        **kwargs,
                    )
                    return response
                except openai.RateLimitError:
                    retries -= 1
                    if retries == 0:
                        return "Error: Rate limit reached and retries exhausted."
                    logger.info("Sleep for 5 seconds for API limit.")
                    await asyncio.sleep(5)
                except Exception as e:
                    logger.error("Error: %s", e)
                    return f"Error processing content: {e}"

        # If prompt is in chat format, convert it to text format
        prompt_text = prompt
        if isinstance(prompt, list) and all(isinstance(msg, dict) for msg in prompt):
            prompt_text = self.chat_parser.parse(prompt, add_generation_prompt=True, is_first_msg=True)

        response = await get_response(prompt_text)
        # if isinstance(response, Completion):
        #     response = response.choices[0].text
        # If SDK typed object:
        if isinstance(response, Completion):
            ch = response.choices[0]
            text = ch.text
            lp = getattr(ch, "logprobs", None)

            response_logprobs = []
            if lp and getattr(lp, "token_logprobs", None):
                # For Completions, token_logprobs align to the completion tokens directly (echo=False)
                response_logprobs = list(lp.token_logprobs)

            if kwargs.get("return_logprobs", True):
                return {"text": text, "response_logprobs": response_logprobs}
            return text
        return response

    async def run_agent_trajectory_async(self, idx, application_id, seed=0, mode="Text", **kwargs):
        """Run a single agent's trajectory asynchronously"""
        agent = self.agents[idx]
        env = self.envs[idx]

        termination_reason = None
        prompt_token_len = 0
        prompt_tokens = []
        response_token_len = 0
        response_tokens = []
        response_masks = []
        response_logprobs_all = []  # <-- collect per-token logprobs aligned with response_tokens
        total_time = 0.0
        reward_time = None
        llm_time = 0.0
        env_time = 0.0
        reward = 0.0

        # for step return
        episode_steps = []

        # Reset environment with the task using the executor
        loop = asyncio.get_event_loop()
        observation, info = await loop.run_in_executor(self.executor, env.reset)
        info["max_steps"] = self.max_steps

        # Reset agent
        agent.reset()
        # Update agent internal state from environment.
        agent.update_from_env(
            observation=observation,  # Raw observation from environment
            reward=0.0,
            done=False,
            info=info,
        )
    
        messages = agent.chat_completions
        prompt_tokens, _ = convert_messages_to_tokens_and_masks(
            messages,
            tokenizer=self.tokenizer,
            parser=self.chat_parser,
            contains_first_msg=True,
            contains_generation_msg=True,
        )
        prompt_token_len = len(prompt_tokens)
        if prompt_token_len > self.max_prompt_length:
            agent.reset()
            raise Exception(
                f"Trajectory {idx}: initial prompt length {prompt_token_len} already exceeded "
                f"max_prompt_length {self.max_prompt_length}, retrying"
            )

        for step_idx in range(self.max_steps):
            # Build the prompt/messages for this step
            prompt_messages = agent.chat_completions.copy()

            # Decide how many tokens we can still emit
            if not self.enforce_max_prompt_length:
                max_tokens = self.max_response_length - response_token_len
            else:
                max_tokens = self.max_response_length
                # Enforce per-step prompt length if requested
                prompt_str = self.chat_parser.parse(prompt_messages, add_generation_prompt=True, is_first_msg=True)
                prompt_len = len(self.tokenizer.encode(prompt_str, add_special_tokens=False))
                if prompt_len > self.max_prompt_length:
                    termination_reason = "PROMPT_TRUNCATION"
                    break

            kwargs["max_tokens"] = max_tokens

            # === LLM call (unwrap {text, response_logprobs} if returned) ===
            start_time = time.time()
            model_out = await self.get_model_response(prompt_messages, application_id, **kwargs)
            delta_time = time.time() - start_time
            llm_time += delta_time
            total_time += delta_time

            if isinstance(model_out, dict):
                resp_text = model_out.get("text", "")
                resp_lps = model_out.get("response_logprobs", None)  # list[float] aligned to the assistant completion tokens
            else:
                resp_text = model_out
                resp_lps = None

            # Record step prompt/response text (for Step mode / debugging)
            prompt_response_pair = {
                "prompt": self.chat_parser.parse(prompt_messages, add_generation_prompt=True, is_first_msg=True),
                "response": resp_text,
            }
            # carry per-step rollout logprobs (aligned/padded later in _transform_agent_steps)
            if resp_lps is not None:
                # ensure JSON-serializable
                prompt_response_pair["response_logprobs"] = [float(x) for x in resp_lps]
            episode_steps.append(prompt_response_pair)

            # === Update agent with the model response ===
            action: Action = agent.update_from_model(resp_text)
            action = action.action

            # Optional: extract think
            thought = ""
            if "</think>" in resp_text:
                try:
                    think_text = resp_text.split("</think>")[0]
                    if "<think>" in think_text:
                        thought = think_text.split("<think>")[1].strip()
                    else:
                        thought = think_text.strip()
                except:
                    thought = ""

            # === Step the environment ===
            start_time = time.time()
            try:
                next_observation, reward, done, info = await asyncio.wait_for(
                    loop.run_in_executor(self.executor, env.step, action),
                    timeout=(self.trajectory_timeout - total_time),
                )
            except asyncio.TimeoutError:
                termination_reason = "ENV_TIMEOUT"
                if step_idx == 0:
                    colorful_print(
                        f"Warning: Trajectory {idx} completed due to: {termination_reason} before able to perform "
                        f"1 complete action. This might cause unexpected behavior. Consider increasing trajectory timeout limit.\n",
                        "red",
                    )
                reward = 0
                cur_step = agent.get_current_state()
                done = True
                cur_step.done = done
                break

            delta_time = time.time() - start_time
            env_time += delta_time
            total_time += delta_time
            info["max_steps"] = self.max_steps
            info["cur_tokens"] = response_token_len

            # Update agent internal state from env
            agent.update_from_env(
                observation=next_observation,
                reward=reward,
                done=done,
                info=info,
            )
            cur_step = agent.get_current_state()
            cur_step.reward = reward
            cur_step.done = done
            cur_step.info.update(info)

            # Attach thought if available
            if hasattr(cur_step, 'thought'):
                cur_step.thought = thought
            else:
                setattr(cur_step, 'thought', thought)

            # === Optional extras copied from observation ===
            if hasattr(agent, '_current_obs') and isinstance(agent._current_obs, dict):
                # find the last agent chat completion with "role" == "user"
                last_user_msg = None
                for msg in reversed(agent.chat_completions):
                    if msg["role"] == "user":
                        last_user_msg = msg
                        break
                if not hasattr(cur_step, 'extras') or cur_step.extras is None:
                    cur_step.extras = {}

                obs_solver_prompt = agent._current_obs.get("solver_prompt", "")
                obs_solver_full_output = agent._current_obs.get("solver_full_output", "")
                obs_solver_code = agent._current_obs.get("solver_output", "")
                obs_verifier_results = agent._current_obs.get("verifier_results", "")
                obs_passed_tests = agent._current_obs.get("passed_tests", 0)
                obs_total_tests = agent._current_obs.get("total_tests", 0)
                obs_solved = agent._current_obs.get("solved", False)

                cur_step.extras.update({
                    "solver_prompt": obs_solver_prompt,
                    "solver_full_output": obs_solver_full_output,
                    "solver_code": obs_solver_code,
                    "verifier_results": obs_verifier_results,
                    "passed_tests": obs_passed_tests,
                    "total_tests": obs_total_tests,
                    "solved": obs_solved,
                    "context_manager_prompt": self.chat_parser.parse([last_user_msg], add_generation_prompt=True, is_first_msg=True) if last_user_msg else "",
                })

            # === Tokenize the latest assistant + env/user messages ===
            chat_completions_messages = agent.chat_completions
            assistant_message, env_messages = get_recent_assistant_user_messages(chat_completions_messages)

            assert assistant_message is not None or mode != "Token", \
                "Assistant messages is none when accumulating token trajectories which should be conversations."
            assert env_messages is not None or mode != "Token", \
                "Environment messages is none when accumulating token trajectories which should be conversations."

            assistant_msg_tokens, assistant_msg_masks = [], []
            env_msg_tokens, env_msg_masks = [], []
            if assistant_message:
                assistant_msg_tokens, assistant_msg_masks = convert_messages_to_tokens_and_masks(
                    [assistant_message],
                    tokenizer=self.tokenizer,
                    parser=self.chat_parser,
                    contains_first_msg=False,
                    contains_generation_msg=False,
                )
            if env_messages:
                env_msg_tokens, env_msg_masks = convert_messages_to_tokens_and_masks(
                    env_messages,
                    tokenizer=self.tokenizer,
                    parser=self.chat_parser,
                    contains_first_msg=False,
                    contains_generation_msg=True,
                )

            # === Update total response token count (assistant + env) ===
            step_assist_len = len(assistant_msg_tokens)
            step_env_len = len(env_msg_tokens)
            response_token_len += step_assist_len + step_env_len

            # Build aligned logprobs for assistant tokens; zeros for env tokens
            if resp_lps is None:
                assist_lps = [0.0] * step_assist_len
            else:
                # pad/trim to match assistant tokens
                if len(resp_lps) > step_assist_len:
                    assist_lps = resp_lps[:step_assist_len]
                elif len(resp_lps) < step_assist_len:
                    assist_lps = resp_lps + [0.0] * (step_assist_len - len(resp_lps))
                else:
                    assist_lps = resp_lps
            combined_step_lps = assist_lps + [0.0] * step_env_len

            # === If we've exceeded max_response_length, truncate tokens/masks/logprobs consistently ===
            if not self.enforce_max_prompt_length and response_token_len >= self.max_response_length:
                truncation_length = self.max_response_length - response_token_len
                # Build combined step lists (assistant + env) and truncate identically
                combined_tokens = (assistant_msg_tokens + env_msg_tokens)
                combined_masks  = (assistant_msg_masks + env_msg_masks)

                if truncation_length < 0:
                    truncated_tokens = combined_tokens[:truncation_length]
                    truncated_masks  = combined_masks[:truncation_length]
                    truncated_lps    = combined_step_lps[:truncation_length]
                else:
                    truncated_tokens = combined_tokens
                    truncated_masks  = combined_masks
                    truncated_lps    = combined_step_lps

                response_tokens.extend(truncated_tokens)
                response_masks.extend(truncated_masks)
                response_logprobs_all.extend(truncated_lps)

                cur_step = agent.get_current_state()
                if response_token_len - step_env_len > self.max_response_length:
                    cur_step.reward = 0.0
                cur_step.done = True
                termination_reason = "TRUNCATION"
                break

            # === Normal (non-truncated) append ===
            response_tokens.extend(assistant_msg_tokens)
            response_masks.extend(assistant_msg_masks)
            response_logprobs_all.extend(assist_lps)

            observation = next_observation

            if total_time >= self.trajectory_timeout:
                termination_reason = "TIMEOUT"
                cur_step = agent.get_current_state()
                done = True
                cur_step.done = done
                break

            if done:
                termination_reason = "ENV_DONE"
                # Still append env messages for visibility/faithful transcript (with zero logprobs)
                response_tokens.extend(env_msg_tokens)
                response_masks.extend(env_msg_masks)
                response_logprobs_all.extend([0.0] * step_env_len)
                break

            # Not done: append env messages as part of the response stream
            response_tokens.extend(env_msg_tokens)
            response_masks.extend(env_msg_masks)
            response_logprobs_all.extend([0.0] * step_env_len)

            if step_idx == self.max_steps - 1:
                termination_reason = "MAX_STEPS"

        # Optionally mask overlong trajectories
        masked_out = False
        if self.overlong_filter:
            if termination_reason in {"TRUNCATION", "MAX_STEPS", "TIMEOUT"}:
                response_masks = [0] * len(response_masks)
                masked_out = True

        # Final reward if available
        if hasattr(env, "compute_final_reward") and not masked_out:
            cur_step = agent.get_current_state()
            start_time = time.time()
            reward = await loop.run_in_executor(self.executor, env.compute_final_reward)
            reward_time = time.time() - start_time
            cur_step.reward = reward

        # Close env
        await loop.run_in_executor(self.executor, env.close)

        if termination_reason:
            color = "green" if reward > 0 else "yellow"
            colorful_print(
                f"Trajectory {idx} completed due to: {termination_reason}. Reward is {reward}. \n",
                color,
            )
            if masked_out:
                colorful_print(f"Trajectory {idx} is masked out due to overlong filter.", "red")

        trajectory: Trajectory = agent.trajectory
        compute_trajectory_reward(trajectory)
        compute_mc_return(trajectory, gamma=self.gamma)

        if mode == "Text":
            return trajectory
        elif mode == "Token":
            # Sanity: lengths must match
            assert len(response_tokens) == len(response_masks) == len(response_logprobs_all), \
                f"Token/mask/logprob length mismatch: {len(response_tokens)} vs {len(response_masks)} vs {len(response_logprobs_all)}"

            token_result = {
                "prompt_tokens": torch.tensor(prompt_tokens, dtype=torch.long),
                "response_tokens": torch.tensor(response_tokens, dtype=torch.long),
                "response_masks": torch.tensor(response_masks, dtype=torch.long),
                "response_logprobs": torch.tensor(response_logprobs_all, dtype=torch.float32),  # <-- NEW
                "trajectory_reward": trajectory.reward,
                "idx": env.idx,
                "chat_completions": agent.chat_completions,
                "metrics": {
                    "steps": len(trajectory.steps),
                    "reward_time": reward_time,
                    "env_time": env_time,
                    "llm_time": llm_time,
                    "total_time": total_time,
                },
            }
            return token_result
        elif mode == "Conversation":
            return agent.chat_completions
        elif mode == "Step":
            steps_result = {
                "steps": episode_steps,
                "trajectory_reward": trajectory.reward,
                "idx": env.idx,
                "mc_returns": [step.mc_return for step in trajectory.steps][: len(episode_steps)],
            }
            return steps_result

    async def run_agent_trajectory_with_retry(self, idx, application_id, seed=0, mode="Text", **kwargs):
        for _ in range(self.retry_limit):
            try:
                return await asyncio.wait_for(self.run_agent_trajectory_async(idx, application_id=application_id, seed=seed, mode=mode, **kwargs), timeout=7200)
            except Exception:
                traceback.print_exc()
                continue
        traceback.print_exc()
        raise Exception(f"Trajectory {idx} cannot complete. Please check the log message")

    async def trajectory_generator(self, reset_seed=0, timing_raw=None, mode="Text", **kwargs):
        if timing_raw is None:
            timing_raw = {}
        assert all(env is not None and isinstance(env, BaseEnv) for env in self.envs), "All environments must be inheriting from BaseEnv"
        assert all(env.is_multithread_safe() for env in self.envs), "All environments must be multithread safe for async engine"  # type: ignore
        max_concurrency = self.n_parallel_agents
        self.executor = ThreadPoolExecutor(max_workers=max_concurrency)

        if self.engine_name == "verl":
            # self.rollout_engine.wake_up()
            wake = getattr(self.rollout_engine, "wake_up", None)
            if callable(wake):
                wake()

        async def launch_one_trajectory_task(env_idx: int):
            try:
                application_id = str(uuid.uuid4())
                result = await self.run_agent_trajectory_with_retry(
                    idx=env_idx,
                    application_id=application_id,
                    seed=reset_seed,
                    mode=mode,
                    **kwargs,
                )
            except Exception as e:
                import traceback

                traceback.print_exc()
                raise e
            return result

        # Create all N conceptual tasks. Their execution will be throttled by the semaphore
        # and the availability of agent/env indices.
        tasks_to_run = [launch_one_trajectory_task(i) for i in range(len(self.envs))]

        tasks_completed = 0
        for coro in asyncio.as_completed(tasks_to_run):
            try:
                result = await coro
                tasks_completed += 1
                colorful_print(f"Number of Trajectories {tasks_completed}/{len(self.envs)} completed", "cyan")
                yield result
            except Exception as e:
                raise e

        if self.engine_name == "verl":
            self.rollout_engine.sleep()

        self.executor.shutdown(wait=False, cancel_futures=True)

    async def execute_tasks(self, tasks: list[dict]):
        """
        Run asynchronous interactions between the agent and environment where each agent
        has its own environment instance and can proceed independently.

        Args:
            tasks: List of tasks to process
            max_concurrent: Maximum number of concurrent tasks to process (defaults to self.n_parallel_agents)

        Returns:
            A list of trajectories, one for each task.
        """

        max_concurrent = self.n_parallel_agents

        # Initialize results list to store trajectories for all tasks
        all_trajectories = {}

        # Create a queue of tasks to process
        task_queue = list(enumerate(tasks))
        semaphore = asyncio.Semaphore(max_concurrent)
        index_queue: asyncio.Queue[int] = asyncio.Queue(maxsize=max_concurrent)
        for i in range(max_concurrent):
            index_queue.put_nowait(i)

        # Track completed trajectories
        completed = 0
        total = len(tasks)

        async def sem_wrapper(task_id, task):
            nonlocal completed
            async with semaphore:
                # Get an available index
                index = await index_queue.get()
                try:
                    self.envs[index] = self.env_class.from_dict({**task, **self.env_args})
                    self.agents[index] = self.agent_class(**self.agent_args)
                    assert self.agents[index] is not None and isinstance(self.agents[index], BaseAgent), "Agent is not initalized or not inheriting from BaseAgent"
                    self.agents[index].trajectory.task = task  # type: ignore
                    res = await self.run_agent_trajectory_async(index, application_id=task_id)
                    res.task = task
                    completed += 1
                    colorful_print(f"Progress: {completed}/{total} trajectories completed", "cyan")
                    return task_id, res
                finally:
                    # Put the index back in the queue when done
                    await index_queue.put(index)

        # Run all tasks concurrently
        results = await asyncio.gather(*[sem_wrapper(task_id, task) for task_id, task in task_queue])

        all_trajectories = {task_id: trajectory for task_id, trajectory in results}
        ordered_trajectories = [all_trajectories[i] for i in range(len(all_trajectories))]
        return ordered_trajectories

    def _convert_prompt_verl(self, prompts, **kwargs):
        """
        Given a list of prompts in Chat template, convert to DataProto format in veRL

        Args:
            prompts: List of prompts to convert
            **kwargs: Additional arguments

        Returns:
            DataProto object containing the converted prompts
        """
        from verl.protocol import DataProto, union_two_dict
        from verl.utils.model import compute_position_id_with_mask
        from verl.utils.torch_functional import pad_sequence_to_length

        old_padding_side = self.tokenizer.padding_side
        self.tokenizer.padding_side = "left"

        formatted_prompts = [self.chat_parser.parse(prompt, add_generation_prompt=True, is_first_msg=True) for prompt in prompts]

        # Tokenize the final processed strings
        inputs = self.tokenizer(
            formatted_prompts,
            padding=True,
            return_tensors="pt",
            add_special_tokens=False,
        )
        self.tokenizer.padding_side = old_padding_side

        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]

        # pad to max sizes
        input_ids = pad_sequence_to_length(input_ids, max_seq_len=self.max_prompt_length, pad_token_id=self.tokenizer.pad_token_id, left_pad=True)
        attention_mask = pad_sequence_to_length(attention_mask, max_seq_len=self.max_prompt_length, pad_token_id=0, left_pad=True)
        position_ids = compute_position_id_with_mask(attention_mask)
        batch_dict = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "position_ids": position_ids,
        }
        data = DataProto.from_dict(batch_dict)
        data.non_tensor_batch["formatted_prompts"] = np.array(formatted_prompts)

        # original_batch contains the extra info needed for generation
        if "meta_info" in kwargs and kwargs["meta_info"]:
            meta_info = kwargs["meta_info"]
            # only use the original_batch's meta_info since tensor_batch is from batch_dict and non_tensor_batch is not neeeded
            data.meta_info = union_two_dict(data.meta_info, meta_info)

        return data


class AsyncAgentExecutionEngine(AgentExecutionEngine):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

# import asyncio
# import copy
# import concurrent.futures
# import logging
# import time
# import traceback
# import uuid
# from concurrent.futures import ThreadPoolExecutor

# import numpy as np
# import openai
# import torch
# from openai.types import Completion

# from rllm.agents.agent import Action, BaseAgent, Step, Trajectory
# from rllm.agents.utils import (
#     convert_messages_to_tokens_and_masks,
#     get_recent_assistant_user_messages,
# )
# from rllm.environments.base.base_env import BaseEnv
# from rllm.environments.env_utils import (
#     compute_mc_return,
#     compute_trajectory_reward,
# )
# from rllm.misc import colorful_print
# from rllm.parser.chat_template.parser import ChatTemplateParser
# from rllm.router.router import Router

# logger = logging.getLogger(__name__)


# class AgentExecutionEngine:
#     def __init__(
#         self,
#         engine_name="openai",
#         tokenizer=None,
#         rollout_engine=None,
#         chat_parser=None,
#         n_parallel_agents=1,
#         trajectory_timeout=None,
#         gamma=0.2,
#         api_retries=3,
#         retry_limit=3,
#         max_steps=5,
#         max_response_length=8192,
#         max_prompt_length=1024,
#         config=None,
#         agent_class=None,
#         env_class=None,
#         agent_args=None,
#         rollout_engine_args=None,
#         env_args=None,
#         max_workers=64,
#         enforce_max_prompt_length=False,  # If enabled, applies max_prompt check per step
#         overlong_filter=False,  # Filter for overlong trajectories (i.e. TRUNCATION, MAX_STEPS, TIMEOUT)
#         **kwargs,
#     ):
#         if agent_args is None:
#             agent_args = {}
#         if rollout_engine_args is None:
#             rollout_engine_args = {}
#         if env_args is None:
#             env_args = {}

#         self.config = config
#         self.rollout_engine = rollout_engine
#         self.tokenizer = tokenizer
#         self.engine_name = engine_name
#         self.n_parallel_agents = n_parallel_agents
#         self.overlong_filter = overlong_filter

#         # For interaction
#         self.gamma = gamma
#         self.retry_limit = retry_limit
#         self.api_retries = api_retries
#         self.max_steps = max_steps
#         self.max_response_length = max_response_length
#         self.max_prompt_length = max_prompt_length
#         self.enforce_max_prompt_length = enforce_max_prompt_length

#         self.agent_class = agent_class
#         self.agent_args = agent_args
#         self.env_class = env_class
#         self.env_args = env_args

#         self.agents = [None for _ in range(n_parallel_agents)]
#         self.envs = [None for _ in range(n_parallel_agents)]

#         self.trajectory_timeout = trajectory_timeout
#         if not trajectory_timeout:
#             self.trajectory_timeout = int(1e9)

#         if env_class is not None:
#             assert env_class.is_multithread_safe(), "Environment must be multithread safe for async engine"
#         # rollout engine args
#         self.rollout_engine_args = rollout_engine_args
#         self.sampling_params = kwargs.get("sampling_params", {})

#         assert self.engine_name in ["openai", "verl"], "Currently only openai and verl are supported as rollout engine"
#         if self.engine_name == "openai":
#             from openai import AsyncOpenAI

#             self.client = AsyncOpenAI(**self.rollout_engine_args)
#             # Disable httpx INFO logs that show HTTP requests
#             logging.getLogger("httpx").setLevel(logging.WARNING)
#         elif self.engine_name == "verl":
#             # All generation is done via scheduler. Currently only works for verl
#             self.server_addresses = getattr(self.rollout_engine, "server_addresses", [])
#             self.router = Router(config=self.config, tokenizer=self.tokenizer, addresses=self.server_addresses)

#         # Create a thread pool executor for environment interactions (i.e. step, reset, close)
#         self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)

#         if chat_parser is None:
#             self.chat_parser = ChatTemplateParser.get_parser(self.tokenizer, disable_thinking=kwargs.get("disable_thinking", False))
#         else:
#             self.chat_parser = chat_parser

#     async def get_model_response(self, prompt, application_id, **kwargs):
#         """
#         Compute model response asynchronously based on the engine type.

#         This function is multithread safe and routes the request to the appropriate
#         engine-specific handler.

#         Args:
#             prompt: The input prompt to send to the model
#             application_id: Unique identifier for the application
#             **kwargs: Additional arguments to pass to the model

#         Returns:
#             The model's response text

#         Raises:
#             NotImplementedError: If the engine type is not supported
#         """
#         # print("PROMPT: ", prompt)
#         if self.engine_name == "openai":
#             return await self._get_openai_async(prompt, application_id, **kwargs)
#         elif self.engine_name == "verl":
#             return await self._get_verl_async(prompt, application_id, **kwargs)
#         else:
#             raise NotImplementedError(f"Engine type '{self.engine_name}' not supported")

#     def update_envs_and_agents(self, envs, agents):
#         """
#         Update the environments and agents.

#         Args:
#             envs: List of environments to use
#             agents: List of agents to use
#         """
#         assert len(agents) == len(envs), f"Number of agents must equal to number of environments but received, {len(agents)} and {len(envs)}"
#         self.envs = envs
#         # For keeping track of the environment index in the batch.
#         for idx, env in enumerate(envs):
#             env.idx = idx
#         self.agents = agents
#         self.n_parallel_agents = len(envs)

#     async def _get_verl_async(self, prompt, application_id, **kwargs):
#         batch = self._convert_prompt_verl([prompt], **kwargs)

#         if "max_tokens" in kwargs:
#             batch.meta_info["max_tokens"] = kwargs["max_tokens"]

#         output = await self.router.generate_sequences(batch, application_id=application_id, **kwargs)

#         attn = output.batch["attention_mask"][0, self.max_prompt_length :]
#         tokens = output.batch["responses"][0]

#         # Find last index where attention == 1
#         non_pad_indices = (attn == 1).nonzero(as_tuple=True)[0]
#         if len(non_pad_indices) == 0:
#             trimmed = tokens[:0]  # empty
#         else:
#             last_valid_idx = non_pad_indices[-1].item()
#             trimmed = tokens[: last_valid_idx + 1]  # include the last valid token

#         response = self.tokenizer.decode(trimmed, skip_special_tokens=False)

#         pad_token = self.tokenizer.pad_token
#         eos_token = self.tokenizer.eos_token
#         response = response.replace(pad_token, "").replace(eos_token, "")
#         return response

#     async def _get_openai_async(self, prompt, _, **kwargs):
#         """
#         Get action from OpenAI API asynchronously with retry logic.

#         Args:
#             prompt: The input prompt in text format for completions API
#             application_id: Unique identifier for the application (unused for OpenAI)
#             **kwargs: Additional arguments to pass to the OpenAI API

#         Returns:
#             The response from OpenAI API
#         """

#         async def get_response(prompt_text: str):
#             retries = self.api_retries
#             while retries > 0:
#                 try:
#                     response = await self.client.completions.create(
#                         prompt=prompt_text,
#                         timeout=3600,
#                         **self.sampling_params,
#                         **kwargs,
#                     )
#                     return response
#                 except openai.RateLimitError:
#                     retries -= 1
#                     if retries == 0:
#                         return "Error: Rate limit reached and retries exhausted."
#                     logger.info("Sleep for 5 seconds for API limit.")
#                     await asyncio.sleep(5)
#                 except Exception as e:
#                     logger.error("Error: %s", e)
#                     return f"Error processing content: {e}"

#         # If prompt is in chat format, convert it to text format
#         prompt_text = prompt
#         if isinstance(prompt, list) and all(isinstance(msg, dict) for msg in prompt):
#             prompt_text = self.chat_parser.parse(prompt, add_generation_prompt=True, is_first_msg=True)

#         response = await get_response(prompt_text)
#         if isinstance(response, Completion):
#             response = response.choices[0].text
#         return response

#     async def run_agent_trajectory_async(self, idx, application_id, seed=0, mode="Text", **kwargs):
#         """Run a single agent's trajectory asynchronously"""
#         agent = self.agents[idx]
#         env = self.envs[idx]
#         # env_id = env.env_id

#         termination_reason = None
#         prompt_token_len = 0
#         prompt_tokens = []
#         response_token_len = 0
#         response_tokens = []
#         response_masks = []
#         total_time = 0.0
#         reward_time = None
#         llm_time = 0.0
#         env_time = 0.0
#         reward = 0.0

#         # for step return
#         episode_steps = []

#         # Reset environment with the task using the executor
#         loop = asyncio.get_event_loop()
#         observation, info = await loop.run_in_executor(self.executor, env.reset)
#         info["max_steps"] = self.max_steps

#         # Reset agent
#         agent.reset()
#         # Update agent internal state from environment.
#         agent.update_from_env(
#             observation=observation,  # Raw observation from environment
#             reward=0.0,
#             done=False,
#             info=info,
#         )
        
#         messages = agent.chat_completions
#         prompt_tokens, _ = convert_messages_to_tokens_and_masks(messages, tokenizer=self.tokenizer, parser=self.chat_parser, contains_first_msg=True, contains_generation_msg=True)
#         prompt_token_len = len(prompt_tokens)
#         # Note, this should never happen!
#         if prompt_token_len > self.max_prompt_length:
#             agent.reset()
#             raise Exception(f"Trajectory {idx}: initial prompt length {prompt_token_len} already exceeded max_prompt_length {self.max_prompt_length}, retrying")

#         for step_idx in range(self.max_steps):
#             # Get action from agent
#             prompt_messages = agent.chat_completions.copy()
#             # Max remaining tokens left for the response
#             # For enforced max prompt at each step, no need to deduct here
#             if not self.enforce_max_prompt_length:
#                 max_tokens = self.max_response_length - response_token_len
#             else:
#                 max_tokens = self.max_response_length

#                 # since max prompt is enforced, we filter out too long prompts.
#                 prompt_str = self.chat_parser.parse(prompt_messages, add_generation_prompt=True, is_first_msg=True)
#                 prompt_len = len(self.tokenizer.encode(prompt_str, add_special_tokens=False))
#                 if prompt_len > self.max_prompt_length:
#                     termination_reason = "PROMPT_TRUNCATION"
#                     break

#             kwargs["max_tokens"] = max_tokens

#             start_time = time.time()
#             response = await self.get_model_response(prompt_messages, application_id, **kwargs)
#             delta_time = time.time() - start_time
#             llm_time += delta_time
#             total_time += delta_time
#             # Update steps
#             prompt_response_pair = {
#                 "prompt": self.chat_parser.parse(prompt_messages, add_generation_prompt=True, is_first_msg=True),
#                 "response": response,
#             }
#             episode_steps.append(prompt_response_pair)

#             # Update agent with model response
#             action: Action = agent.update_from_model(response)
#             action = action.action
            
#             # Extract thinking from response if present
#             thought = ""
#             if "</think>" in response:
#                 try:
#                     # Get everything before </think>
#                     think_text = response.split("</think>")[0]
#                     # Strip <think> tag if it exists
#                     if "<think>" in think_text:
#                         thought = think_text.split("<think>")[1].strip()
#                     else:
#                         thought = think_text.strip()
#                 except:
#                     thought = ""

#             # Take step in environment using the executor
#             start_time = time.time()

#             try:
#                 next_observation, reward, done, info = await asyncio.wait_for(loop.run_in_executor(self.executor, env.step, action), timeout=(self.trajectory_timeout - total_time))
#             except asyncio.TimeoutError:
#                 termination_reason = "ENV_TIMEOUT"
#                 if step_idx == 0:
#                     colorful_print(f"Warning: Trajectory {idx} completed due to: {termination_reason} before able to perform 1 complete action. This might cause unexpected behavior. Consider increasing trajectory timeout limit.\n", "red")
#                 reward = 0

#                 cur_step = agent.get_current_state()
#                 done = True
#                 cur_step.done = done
#                 break

#             delta_time = time.time() - start_time
#             env_time += delta_time
#             total_time += delta_time
#             info["max_steps"] = self.max_steps
#             info["cur_tokens"] = response_token_len

#             # Update agent internal state.
#             agent.update_from_env(
#                 observation=next_observation,
#                 reward=reward,
#                 done=done,
#                 info=info,
#             )

#             cur_step = agent.get_current_state()
#             cur_step.reward = reward
#             cur_step.done = done
#             cur_step.info.update(info)
            
#             # Add thought field to the step
#             if hasattr(cur_step, 'thought'):
#                 cur_step.thought = thought
#             else:
#                 # If thought attribute doesn't exist, add it dynamically
#                 setattr(cur_step, 'thought', thought)
            
#             # Update step extras with solver information from the observation
#             if hasattr(agent, '_current_obs') and isinstance(agent._current_obs, dict):
#                 # find the last agent chat completion with "role" == "user"
#                 last_user_msg = None
#                 for msg in reversed(agent.chat_completions):
#                     if msg["role"] == "user":
#                         last_user_msg = msg
#                         break
#                 context_manager_prompt = last_user_msg
#                 if not hasattr(cur_step, 'extras') or cur_step.extras is None:
#                     cur_step.extras = {}

#                 # Base fields from current observation
#                 obs_solver_prompt = agent._current_obs.get("solver_prompt", "")
#                 print(f"[agent_execution_engine] Obs solver prompt: {obs_solver_prompt}")
#                 obs_solver_full_output = agent._current_obs.get("solver_full_output", "")
#                 obs_solver_code = agent._current_obs.get("solver_output", "")
#                 obs_verifier_results = agent._current_obs.get("verifier_results", "")
#                 obs_passed_tests = agent._current_obs.get("passed_tests", 0)
#                 obs_total_tests = agent._current_obs.get("total_tests", 0)
#                 obs_solved = agent._current_obs.get("solved", False)
#                 obs_round_idx = agent._current_obs.get("round_idx", 0)

#                 cur_step.extras.update({
#                     "solver_prompt": obs_solver_prompt,
#                     "solver_full_output": obs_solver_full_output,
#                     "solver_code": obs_solver_code,
#                     "verifier_results": obs_verifier_results,
#                     "passed_tests": obs_passed_tests,
#                     "total_tests": obs_total_tests,
#                     "solved": obs_solved,
#                     "context_manager_prompt": self.chat_parser.parse([last_user_msg], add_generation_prompt=True, is_first_msg=True),
#                 })

#             chat_completions_messages = agent.chat_completions
#             assistant_message, env_messages = get_recent_assistant_user_messages(chat_completions_messages)

#             # Check and convert to tokens if necessary
#             assert assistant_message is not None or mode != "Token", "Assistant messages is none when accumulating token trajectories which should be conversations. This should not happen."
#             assert env_messages is not None or mode != "Token", "Environment messages is none when accumulating token trajectories which should be conversations. This should not happen."
#             assistant_msg_tokens, assistant_msg_masks = [], []
#             env_msg_tokens, env_msg_masks = [], []
#             if assistant_message:
#                 assistant_msg_tokens, assistant_msg_masks = convert_messages_to_tokens_and_masks([assistant_message], tokenizer=self.tokenizer, parser=self.chat_parser, contains_first_msg=False, contains_generation_msg=False)
#             if env_messages:
#                 env_msg_tokens, env_msg_masks = convert_messages_to_tokens_and_masks(env_messages, tokenizer=self.tokenizer, parser=self.chat_parser, contains_first_msg=False, contains_generation_msg=True)

#             # Update repsonse token length
#             response_token_len += len(assistant_msg_tokens) + len(env_msg_tokens)
#             # Reached maximum number of tokens for the trajectory
#             if not self.enforce_max_prompt_length and response_token_len >= self.max_response_length:
#                 # Truncation length
#                 truncation_length = self.max_response_length - response_token_len
#                 # Truncate the response and masks
#                 if truncation_length < 0:
#                     truncated_response_tokens = (assistant_msg_tokens + env_msg_tokens)[:truncation_length]
#                     truncated_response_masks = (assistant_msg_masks + env_msg_masks)[:truncation_length]
#                 else:
#                     # Edge case where the response is exactly the max response length.
#                     truncated_response_tokens = assistant_msg_tokens + env_msg_tokens
#                     truncated_response_masks = assistant_msg_masks + env_msg_masks
#                 # Update token collections
#                 response_tokens.extend(truncated_response_tokens)
#                 response_masks.extend(truncated_response_masks)

#                 cur_step = agent.get_current_state()
#                 if response_token_len - len(env_msg_tokens) > self.max_response_length:
#                     cur_step.reward = 0.0
#                 cur_step.done = True
#                 termination_reason = "TRUNCATION"
#                 # handle returning
#                 break

#             # Update the token version of trajectory
#             response_tokens.extend(assistant_msg_tokens)
#             response_masks.extend(assistant_msg_masks)
#             observation = next_observation

#             if total_time >= self.trajectory_timeout:
#                 termination_reason = "TIMEOUT"
#                 cur_step = agent.get_current_state()
#                 done = True
#                 cur_step.done = done
#                 break

#             # Check if episode is done
#             if done:
#                 termination_reason = "ENV_DONE"
#                 break

#             response_tokens.extend(env_msg_tokens)
#             response_masks.extend(env_msg_masks)

#             if step_idx == self.max_steps - 1:
#                 termination_reason = "MAX_STEPS"

#         masked_out = False
#         if self.overlong_filter:
#             if termination_reason == "TRUNCATION" or termination_reason == "MAX_STEPS" or termination_reason == "TIMEOUT":
#                 # Mask out the entire response for overlong trajectories if the reward is 0.
#                 response_masks = [0] * len(response_masks)
#                 masked_out = True

#         if hasattr(env, "compute_final_reward") and not masked_out:
#             cur_step = agent.get_current_state()
#             start_time = time.time()
#             reward = await loop.run_in_executor(self.executor, env.compute_final_reward)
#             reward_time = time.time() - start_time
#             cur_step.reward = reward
#         # Closing environment using the executor.
#         await loop.run_in_executor(self.executor, env.close)
#         if termination_reason:
#             if reward > 0:
#                 color = "green"
#             else:
#                 color = "yellow"
#             colorful_print(
#                 f"Trajectory {idx} completed due to: {termination_reason}. Reward is {reward}. \n",
#                 color,
#             )
#             if masked_out:
#                 colorful_print(f"Trajectory {idx} is masked out due to overlong filter.", "red")

#         trajectory: Trajectory = agent.trajectory
#         # Aggregate final trajectory statistics
#         compute_trajectory_reward(trajectory)
#         compute_mc_return(trajectory, gamma=self.gamma)

#         if mode == "Text":
#             return trajectory
#         elif mode == "Token":
#             token_result = {
#                 "prompt_tokens": torch.tensor(prompt_tokens, dtype=torch.long),
#                 "response_tokens": torch.tensor(response_tokens, dtype=torch.long),
#                 "response_masks": torch.tensor(response_masks, dtype=torch.long),
#                 "trajectory_reward": trajectory.reward,
#                 "idx": env.idx,
#                 "chat_completions": agent.chat_completions,
#                 "metrics": {
#                     # Total number of steps taken in the trajectory
#                     "steps": len(trajectory.steps),
#                     # Time to calculate reward
#                     "reward_time": reward_time,
#                     # Total time spent in environment execution (env.step)
#                     "env_time": env_time,
#                     # Time to calculate response tokens
#                     "llm_time": llm_time,
#                     # Total time spent in the trajectory
#                     "total_time": total_time,
#                 },
#             }
#             return token_result
#         elif mode == "Conversation":
#             return agent.chat_completions
#         elif mode == "Step":
#             steps_result = {
#                 "steps": episode_steps,
#                 "trajectory_reward": trajectory.reward,
#                 "idx": env.idx,
#                 "mc_returns": [step.mc_return for step in trajectory.steps][: len(episode_steps)],
#             }
#             return steps_result

#     async def run_agent_trajectory_with_retry(self, idx, application_id, seed=0, mode="Text", **kwargs):
#         for _ in range(self.retry_limit):
#             try:
#                 return await asyncio.wait_for(self.run_agent_trajectory_async(idx, application_id=application_id, seed=seed, mode=mode, **kwargs), timeout=7200)
#             except Exception:
#                 traceback.print_exc()
#                 continue
#         traceback.print_exc()
#         raise Exception(f"Trajectory {idx} cannot complete. Please check the log message")

#     async def trajectory_generator(self, reset_seed=0, timing_raw=None, mode="Text", **kwargs):
#         if timing_raw is None:
#             timing_raw = {}
#         assert all(env is not None and isinstance(env, BaseEnv) for env in self.envs), "All environments must be inheriting from BaseEnv"
#         assert all(env.is_multithread_safe() for env in self.envs), "All environments must be multithread safe for async engine"  # type: ignore
#         max_concurrency = self.n_parallel_agents
#         self.executor = ThreadPoolExecutor(max_workers=max_concurrency)

#         if self.engine_name == "verl":
#             # self.rollout_engine.wake_up()
#             wake = getattr(self.rollout_engine, "wake_up", None)
#             if callable(wake):
#                 wake()

#         async def launch_one_trajectory_task(env_idx: int):
#             try:
#                 application_id = str(uuid.uuid4())
#                 result = await self.run_agent_trajectory_with_retry(
#                     idx=env_idx,
#                     application_id=application_id,
#                     seed=reset_seed,
#                     mode=mode,
#                     **kwargs,
#                 )
#             except Exception as e:
#                 import traceback

#                 traceback.print_exc()
#                 raise e
#             return result

#         # Create all N conceptual tasks. Their execution will be throttled by the semaphore
#         # and the availability of agent/env indices.
#         tasks_to_run = [launch_one_trajectory_task(i) for i in range(len(self.envs))]

#         tasks_completed = 0
#         for coro in asyncio.as_completed(tasks_to_run):
#             try:
#                 result = await coro
#                 tasks_completed += 1
#                 colorful_print(f"Number of Trajectories {tasks_completed}/{len(self.envs)} completed", "cyan")
#                 yield result
#             except Exception as e:
#                 raise e

#         if self.engine_name == "verl":
#             self.rollout_engine.sleep()

#         self.executor.shutdown(wait=False, cancel_futures=True)

#     async def execute_tasks(self, tasks: list[dict]):
#         """
#         Run asynchronous interactions between the agent and environment where each agent
#         has its own environment instance and can proceed independently.

#         Args:
#             tasks: List of tasks to process
#             max_concurrent: Maximum number of concurrent tasks to process (defaults to self.n_parallel_agents)

#         Returns:
#             A list of trajectories, one for each task.
#         """

#         max_concurrent = self.n_parallel_agents

#         # Initialize results list to store trajectories for all tasks
#         all_trajectories = {}

#         # Create a queue of tasks to process
#         task_queue = list(enumerate(tasks))
#         semaphore = asyncio.Semaphore(max_concurrent)
#         index_queue: asyncio.Queue[int] = asyncio.Queue(maxsize=max_concurrent)
#         for i in range(max_concurrent):
#             index_queue.put_nowait(i)

#         # Track completed trajectories
#         completed = 0
#         total = len(tasks)

#         async def sem_wrapper(task_id, task):
#             nonlocal completed
#             async with semaphore:
#                 # Get an available index
#                 index = await index_queue.get()
#                 try:
#                     self.envs[index] = self.env_class.from_dict({**task, **self.env_args})
#                     self.agents[index] = self.agent_class(**self.agent_args)
#                     assert self.agents[index] is not None and isinstance(self.agents[index], BaseAgent), "Agent is not initalized or not inheriting from BaseAgent"
#                     self.agents[index].trajectory.task = task  # type: ignore
#                     res = await self.run_agent_trajectory_async(index, application_id=task_id)
#                     res.task = task
#                     completed += 1
#                     colorful_print(f"Progress: {completed}/{total} trajectories completed", "cyan")
#                     return task_id, res
#                 finally:
#                     # Put the index back in the queue when done
#                     await index_queue.put(index)

#         # Run all tasks concurrently
#         results = await asyncio.gather(*[sem_wrapper(task_id, task) for task_id, task in task_queue])

#         all_trajectories = {task_id: trajectory for task_id, trajectory in results}
#         ordered_trajectories = [all_trajectories[i] for i in range(len(all_trajectories))]
#         return ordered_trajectories

#     def _convert_prompt_verl(self, prompts, **kwargs):
#         """
#         Given a list of prompts in Chat template, convert to DataProto format in veRL

#         Args:
#             prompts: List of prompts to convert
#             **kwargs: Additional arguments

#         Returns:
#             DataProto object containing the converted prompts
#         """
#         from verl.protocol import DataProto, union_two_dict
#         from verl.utils.model import compute_position_id_with_mask
#         from verl.utils.torch_functional import pad_sequence_to_length

#         old_padding_side = self.tokenizer.padding_side
#         self.tokenizer.padding_side = "left"

#         formatted_prompts = [self.chat_parser.parse(prompt, add_generation_prompt=True, is_first_msg=True) for prompt in prompts]

#         # Tokenize the final processed strings
#         inputs = self.tokenizer(
#             formatted_prompts,
#             padding=True,
#             return_tensors="pt",
#             add_special_tokens=False,
#         )
#         self.tokenizer.padding_side = old_padding_side

#         input_ids = inputs["input_ids"]
#         attention_mask = inputs["attention_mask"]

#         # pad to max sizes
#         input_ids = pad_sequence_to_length(input_ids, max_seq_len=self.max_prompt_length, pad_token_id=self.tokenizer.pad_token_id, left_pad=True)
#         attention_mask = pad_sequence_to_length(attention_mask, max_seq_len=self.max_prompt_length, pad_token_id=0, left_pad=True)
#         position_ids = compute_position_id_with_mask(attention_mask)
#         batch_dict = {
#             "input_ids": input_ids,
#             "attention_mask": attention_mask,
#             "position_ids": position_ids,
#         }
#         data = DataProto.from_dict(batch_dict)
#         data.non_tensor_batch["formatted_prompts"] = np.array(formatted_prompts)

#         # original_batch contains the extra info needed for generation
#         if "meta_info" in kwargs and kwargs["meta_info"]:
#             meta_info = kwargs["meta_info"]
#             # only use the original_batch's meta_info since tensor_batch is from batch_dict and non_tensor_batch is not neeeded
#             data.meta_info = union_two_dict(data.meta_info, meta_info)

#         return data


# class AsyncAgentExecutionEngine(AgentExecutionEngine):
#     def __init__(self, *args, **kwargs):
#         super().__init__(*args, **kwargs)